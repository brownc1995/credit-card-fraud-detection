{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection\n",
    "\n",
    "The following analysis has been done on Kaggle's [Credit Card Fraud Detection dataset](https://www.kaggle.com/mlg-ulb/creditcardfraud). A particularly helpful resource is TensorFlow's [tutorial](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data) on classification with imbalanced data.\n",
    "\n",
    "All theoretical concepts are covered in my notes on ML/deep learning, which can be found [here](https://github.com/brownc1995/machine-learning-notes/blob/master/an_introduction_to_machine_learning.pdf).\n",
    "\n",
    "## Context\n",
    "It is important that credit card companies are able to recognize \n",
    "fraudulent credit card transactions so that customers are not \n",
    "charged for items that they did not purchase.\n",
    "\n",
    "## Content\n",
    "The [dataset](sample/data) contains transactions made by credit cards in \n",
    "September 2013 by European cardholders. This dataset presents \n",
    "transactions that occurred in two days, where we have 492 frauds \n",
    "out of 284,807 transactions. The dataset is highly unbalanced, \n",
    "the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "The data contains only numerical input variables which are the result\n",
    "of a PCA transformation. Unfortunately, due to confidentiality \n",
    "issues, we cannot provide the original features and more \n",
    "background information about the data. Features `V1`, `V2`,..., \n",
    "`V28` are the principal components obtained with PCA, the only \n",
    "features which have not been transformed with PCA are `Time` \n",
    "and `Amount`. Feature `Time` contains the seconds elapsed between \n",
    "each transaction and the first transaction in the dataset. The \n",
    "feature `Amount` is the transaction amount. This feature can be \n",
    "used for example-dependant cost-senstive learning. Feature 'Class'\n",
    "is the response variable and it takes value `1` in case of __fraud__ \n",
    "and `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:39:34.200103Z",
     "start_time": "2019-11-04T21:39:34.083801Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:39:35.837746Z",
     "start_time": "2019-11-04T21:39:35.770790Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from ccfd import *\n",
    "from ccfd.data import *\n",
    "from ccfd.model import *\n",
    "from ccfd.plot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:39:37.157470Z",
     "start_time": "2019-11-04T21:39:37.101232Z"
    }
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:39:38.512794Z",
     "start_time": "2019-11-04T21:39:38.440777Z"
    }
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:39:39.802094Z",
     "start_time": "2019-11-04T21:39:39.736532Z"
    }
   },
   "outputs": [],
   "source": [
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data and clean\n",
    "\n",
    "First let's import the dataset and do some initial exploration. We remove the `time` column and split the data into features and target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:39:41.264897Z",
     "start_time": "2019-11-04T21:39:41.115073Z"
    }
   },
   "outputs": [],
   "source": [
    "ccfd_data = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have highly imbalanced data! __Accuray__ will therefore not be a good measure of success for our models. Instead, we shall look mostly at [precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall).\n",
    "\n",
    "So, what does our data look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:39:42.590171Z",
     "start_time": "2019-11-04T21:39:42.512056Z"
    }
   },
   "outputs": [],
   "source": [
    "ccfd_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:39:44.337172Z",
     "start_time": "2019-11-04T21:39:43.853973Z"
    }
   },
   "outputs": [],
   "source": [
    "ccfd_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the `Amount` column covers a __very__ large range. As is common-practice with monetary features in financial data, let's transform this column using a log-transformation. Add a very small value pre-transform to avoid any undefined values where `Amount` equals 0. This column has already been added to the `.pkl` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:39:45.516903Z",
     "start_time": "2019-11-04T21:39:45.459785Z"
    }
   },
   "outputs": [],
   "source": [
    "ccfd_data['log_amount'] = np.log(ccfd_data.amount + 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data into training, validation and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:39:46.839171Z",
     "start_time": "2019-11-04T21:39:46.615253Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, train_target, val_data, val_target, test_data, test_target = train_val_test_split(ccfd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also normalise each of the features as they are each in vastly different ranges with different mean/variance. We normalise the input features using the sklearn `StandardScaler`.\n",
    "\n",
    "Note: We fit the `StandardScalar` using only `train_data` to be sure the model is not peeking at the validation or test sets. The validation and test sets are normalised using the same transformation as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:39:48.128503Z",
     "start_time": "2019-11-04T21:39:47.951353Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = scale_data(train_data, val_data, test_data)\n",
    "train_len, val_len, test_len = len(train_data), len(val_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're pretty much ready for analysis. Our data is in the following state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:39:49.325105Z",
     "start_time": "2019-11-04T21:39:49.263685Z"
    }
   },
   "outputs": [],
   "source": [
    "log_shapes(train_data, train_target, val_data, val_target, test_data, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we convert our dataframes into `tf.data.Dataset` types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:39:50.622380Z",
     "start_time": "2019-11-04T21:39:50.542004Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = make_all_datasets(\n",
    "    train_data,\n",
    "    train_target,\n",
    "    val_data,\n",
    "    val_target,\n",
    "    test_data,\n",
    "    test_target\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots\n",
    "\n",
    "Below are two particularly interesting plots that can be found in the TensorFlow [tutorial](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#look_at_the_data_distribution).\n",
    "\n",
    "We compare the distributions of the positive (fraudulent) and negative examples over a few features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:39:54.023282Z",
     "start_time": "2019-11-04T21:39:51.765650Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_pos_neg(train_data, train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T09:08:53.354692Z",
     "start_time": "2019-11-02T09:08:53.268195Z"
    }
   },
   "source": [
    " Good questions to ask yourself at this point are:\n",
    "- Do these distributions make sense?\n",
    "    - __Yes__ - you've normalized the input and these are mostly concentrated in the $\\pm$2 range.\n",
    "- Can you see the difference between the ditributions?\n",
    "    - __Yes__ - the positive examples contain a much higher rate of extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Network\n",
    "\n",
    "We define a function that creates a simple neural network with a series of densely connected hidden layers followed by dropout layers to reduce overfitting, and an output sigmoid layer that returns the probability of a transaction being fraudulent. We use a dropout rate of 0.5 as recommended in [Hinton (2012)](https://arxiv.org/pdf/1207.0580.pdf). We also use [batch nomalisation](https://en.wikipedia.org/wiki/Batch_normalization) between each dense layer to improve efficiency of learning.\n",
    "\n",
    "We use the [Adam](https://arxiv.org/pdf/1412.6980.pdf) optimiser, and binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "First, we build the model and show a summary of its architecture.\n",
    "\n",
    "We know the dataset is imbalanced and so we set the output layer's bias to reflect that (See [A Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/): \"init well\"). This can help with initial convergence. See [here](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#optional_set_the_correct_initial_bias) for details on the calculation involved. \n",
    "\n",
    "It turns out the initial loss is about __50 times less__ than if we used a naive initilization. This way the model doesn't need to spend the first few epochs just learning that positive examples are unlikely. This also makes it easier to read plots of the loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:03:23.981441Z",
     "start_time": "2019-11-04T21:03:23.880821Z"
    }
   },
   "outputs": [],
   "source": [
    "initial_bias = calc_initial_bias(ccfd_data)\n",
    "input_shape = train_data.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:03:25.393154Z",
     "start_time": "2019-11-04T21:03:23.984595Z"
    }
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    input_shape=input_shape, \n",
    "    output_bias=initial_bias\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:03:25.499119Z",
     "start_time": "2019-11-04T21:03:25.395565Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to fit our model. The metrics used are listed below.\n",
    "\n",
    "- __false negatives__ (FN) and __false positives__ (FP) are samples that were incorrectly classified;\n",
    "- __true negatives__ (TN) and __true positives__ (TP) are samples that were correctly classified;\n",
    "- __accuracy__ is the percentage of examples correctly classified;\n",
    "- __precision__ is the percentage of __predicted__ positives that were correctly classified, \n",
    "$$\\frac{TP}{TP+FP};$$ \n",
    "- __recall__ is the percentage of __actual__ positives that were correctly classified,\n",
    "$$\\frac{TP}{TP+FN};$$\n",
    "- __AUC__ refers to the __Area Under the Curve__ of a __Receiver Operating Characteristic__ [ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) curve. This metric is equal to the probability that a classifier will rank a random positive sample higher than than a random negative sample.\n",
    "\n",
    "Note: Accuracy is not a helpful metric for this task. __You can attain 99.827% accuracy on this task by just predicting False all the time__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:04:22.065030Z",
     "start_time": "2019-11-04T21:03:25.581843Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = fit_model(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:04:23.930892Z",
     "start_time": "2019-11-04T21:04:22.067224Z"
    }
   },
   "outputs": [],
   "source": [
    "baseline_results = model.evaluate(\n",
    "    test_dataset,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:04:30.615367Z",
     "start_time": "2019-11-04T21:04:23.934709Z"
    }
   },
   "outputs": [],
   "source": [
    "train_predictions_baseline = model.predict(train_dataset, steps=train_len/BATCH_SIZE)\n",
    "test_predictions_baseline = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:04:30.958759Z",
     "start_time": "2019-11-04T21:04:30.618240Z"
    }
   },
   "outputs": [],
   "source": [
    "log_model_performance(model, baseline_results, test_target, test_predictions_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the precision is relatively high, but the recall and the area under the ROC curve (AUC) aren't as high as you might like. Classifiers often face challenges when trying to maximize both precision and recall, which is especially true when working with imbalanced datasets. It is important to consider the costs of different types of errors in the context of the problem you care about. In this example, a false negative (a fraudulent transaction is missed) may have a financial cost, while a false positive (a transaction is incorrectly flagged as fraudulent) may decrease user happiness.\n",
    "\n",
    "### Class weights\n",
    "\n",
    "The goal is to identify fradulent transactions, but you don't have very many of those positive samples to work with, so you would want to have the classifier heavily weight the few examples that are available. You can do this by passing Keras weights for each class through a parameter. These will cause the model to \"pay more attention\" to examples from an under-represented class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:04:31.078402Z",
     "start_time": "2019-11-04T21:04:30.968070Z"
    }
   },
   "outputs": [],
   "source": [
    "class_weight = set_class_weights(ccfd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try re-training and evaluating the model with class weights to see how that affects the predictions.\n",
    "\n",
    "Note: Using `class_weights` changes the range of the loss. Because of the weighting, the total losses are not comparable between the below model and the one seen above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:04:33.363644Z",
     "start_time": "2019-11-04T21:04:31.089316Z"
    }
   },
   "outputs": [],
   "source": [
    "model_cw = build_model(\n",
    "    input_shape=input_shape, \n",
    "    output_bias=initial_bias\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:05:19.599420Z",
     "start_time": "2019-11-04T21:04:33.502055Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_cw = fit_model(\n",
    "    model_cw,\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    class_weight=class_weight\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:05:20.700902Z",
     "start_time": "2019-11-04T21:05:19.601595Z"
    }
   },
   "outputs": [],
   "source": [
    "cw_results = model_cw.evaluate(\n",
    "    test_dataset,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:05:26.620861Z",
     "start_time": "2019-11-04T21:05:20.702959Z"
    }
   },
   "outputs": [],
   "source": [
    "train_predictions_cw = model_cw.predict(train_dataset, steps=train_len/BATCH_SIZE)\n",
    "test_predictions_cw = model_cw.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:05:26.868332Z",
     "start_time": "2019-11-04T21:05:26.623281Z"
    }
   },
   "outputs": [],
   "source": [
    "log_model_performance(model_cw, cw_results, test_target, test_predictions_cw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling\n",
    "A related approach would be to resample the dataset by oversampling the minority class. The easiest way to produce balanced examples is to start with a positive and a negative dataset, and merge them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:05:27.036592Z",
     "start_time": "2019-11-04T21:05:26.872763Z"
    }
   },
   "outputs": [],
   "source": [
    "pos_data, pos_target, neg_data, neg_target = pos_neg_data(train_data, train_target)\n",
    "pos_dataset, neg_dataset = make_datasets_pos_neg(pos_data, pos_target, neg_data, neg_target)\n",
    "resampled_dataset = resample_dataset(pos_dataset, neg_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-02T20:39:40.609169Z",
     "start_time": "2019-11-02T20:39:40.511016Z"
    }
   },
   "source": [
    "To use this dataset, we'll need the number of steps per epoch.\n",
    "\n",
    "The definition of \"epoch\" in this case is less clear. Say it's the number of batches required to see each negative example once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:05:27.100006Z",
     "start_time": "2019-11-04T21:05:27.040817Z"
    }
   },
   "outputs": [],
   "source": [
    "resampled_steps_per_epoch = resample_steps_per_epoch(ccfd_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try training the model with the resampled data set instead of using class weights to see how these methods compare.\n",
    "\n",
    "Note: Because the data was balanced by replicating the positiver examples, the total dataset size is larger, and each epoch runs for more training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:05:28.522413Z",
     "start_time": "2019-11-04T21:05:27.102134Z"
    }
   },
   "outputs": [],
   "source": [
    "model_rs = build_model(\n",
    "    input_shape=input_shape, \n",
    "    output_bias=initial_bias\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:06:29.081533Z",
     "start_time": "2019-11-04T21:05:28.528815Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_rs = fit_model(\n",
    "    model_rs,\n",
    "    resampled_dataset,\n",
    "    val_dataset,\n",
    "    steps_per_epoch=resampled_steps_per_epoch,\n",
    "    epochs=EPOCHS,\n",
    "    resampled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the training process were considering the whole dataset on each gradient update, this oversampling would be basically identical to the class weighting.\n",
    "\n",
    "But when training the model batch-wise, as you did here, the oversampled data provides a smoother gradient signal: Instead of each positive example being shown in one batch with a large weight, they're shown in many different batches each time with a small weight.\n",
    "\n",
    "This smoother gradient signal makes it easier to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:06:30.392401Z",
     "start_time": "2019-11-04T21:06:29.084446Z"
    }
   },
   "outputs": [],
   "source": [
    "rs_results = model_rs.evaluate(\n",
    "    test_dataset,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:06:35.387867Z",
     "start_time": "2019-11-04T21:06:30.394398Z"
    }
   },
   "outputs": [],
   "source": [
    "train_predictions_rs = model_rs.predict(train_dataset, steps=train_len/BATCH_SIZE)\n",
    "test_predictions_rs = model_rs.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:06:35.681805Z",
     "start_time": "2019-11-04T21:06:35.390097Z"
    }
   },
   "outputs": [],
   "source": [
    "log_model_performance(model_rs, rs_results, test_target, test_predictions_rs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot some ROC curves to illustrate the success of each of the above three networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T21:08:51.999911Z",
     "start_time": "2019-11-04T21:08:51.555011Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_roc_all(\n",
    "    train_target,\n",
    "    test_target,\n",
    "    train_predictions_baseline,\n",
    "    test_predictions_baseline,\n",
    "    train_predictions_cw,\n",
    "    test_predictions_cw,\n",
    "    train_predictions_rs,\n",
    "    test_predictions_rs\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
